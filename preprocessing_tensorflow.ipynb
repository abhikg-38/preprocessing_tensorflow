{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f2353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "d_l=tf.keras.layers.Dense(50,activation='relu',kernel_initializer='he_normal')\n",
    "#he_normal helps solve vanishing gradient for activation functions like ReLU\n",
    "\n",
    "leaky_relu=tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "dense=tf.keras.layers.Dense(50,activation=leaky_relu,kernel_initializer='he_normal')\n",
    "\n",
    "fashion_mnist=tf.keras.datasets.fashion_mnist.load_data()\n",
    "(x_train_full,y_train_full),(x_test_full,y_test_full)=fashion_mnist\n",
    "\n",
    "x_train,y_train=x_train_full[:-5000],y_train_full[:-5000]\n",
    "x_valid,y_valid=x_train_full[-5000:],y_train_full[-5000:]\n",
    "\n",
    "#BATCH NORMALIZATION\n",
    "#here we add BN layer after every hidden layer activation function\n",
    "model=tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28,28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300,activation='relu',kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100,activation='relu',kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10,activation='softmax')\n",
    "    \n",
    "])\n",
    "print('Model summary:')\n",
    "print(model.summary())\n",
    "#first BN layer has 3136 as 4x784(each BN layer has 4 parameters)\n",
    "\n",
    "#GRADIENT CLIPPING clips gradient to prevent exploding gradients problem\n",
    "#optimizer=tf.keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics='accuracy',optimizer='SGD')\n",
    "#optimizer clips gradient between -1.0 and 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e881fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 12s 6ms/step - loss: 0.5618 - accuracy: 0.8055 - val_loss: 0.4135 - val_accuracy: 0.8574\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4135 - accuracy: 0.8534 - val_loss: 0.3687 - val_accuracy: 0.8662\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3690 - accuracy: 0.8674 - val_loss: 0.3573 - val_accuracy: 0.8702\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3461 - accuracy: 0.8739 - val_loss: 0.3471 - val_accuracy: 0.8752\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3258 - accuracy: 0.8812 - val_loss: 0.3317 - val_accuracy: 0.8774\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3085 - accuracy: 0.8890 - val_loss: 0.3307 - val_accuracy: 0.8786\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2919 - accuracy: 0.8955 - val_loss: 0.3279 - val_accuracy: 0.8782\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2814 - accuracy: 0.8975 - val_loss: 0.3162 - val_accuracy: 0.8838\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2715 - accuracy: 0.9022 - val_loss: 0.3208 - val_accuracy: 0.8764\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2624 - accuracy: 0.9053 - val_loss: 0.3182 - val_accuracy: 0.8848\n",
      "INFO:tensorflow:Assets written to: keras_fashion\\assets\n"
     ]
    }
   ],
   "source": [
    "#TRANSFER LEARNING\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=[x_valid,y_valid])\n",
    "model.save('keras_fashion',save_format='tf')\n",
    "\n",
    "model_a=tf.keras.models.load_model('keras_fashion')\n",
    "#we are going to use this model to train a binary classifier\n",
    "model_b_on_a=tf.keras.Sequential(model_a.layers[:-1])\n",
    "model_b_on_a.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "#instead you could do transfer learning on model_a clone\n",
    "model_a_clone=tf.keras.models.clone_model(model_a)\n",
    "model_a_clone.set_weights(model_a.get_weights())\n",
    "\n",
    "model_b_on_a=tf.keras.Sequential(model_a_clone.layers[:-1])\n",
    "model_b_on_a.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "for layer in model_b_on_a.layers[:-1]:\n",
    "    layer.trainable=False #freezing layers\n",
    "#freezing layers as initially, newly added output layer has no knowledge of task so gives wrong predictions\n",
    "#this can destabilize model because of backpropogation\n",
    "\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model_b_on_a.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "#we have to compile model everytime we freeze or unfreeze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2036aecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation before using learning rate scheduler\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3402 - accuracy: 0.8815\n",
      "[0.34021487832069397, 0.8815000057220459]\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2361 - accuracy: 0.9155 - val_loss: 0.3138 - val_accuracy: 0.8868 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2338 - accuracy: 0.9158 - val_loss: 0.3069 - val_accuracy: 0.8860 - lr: 7.0795e-04\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2305 - accuracy: 0.9162 - val_loss: 0.3090 - val_accuracy: 0.8870 - lr: 5.0119e-04\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2293 - accuracy: 0.9175 - val_loss: 0.3096 - val_accuracy: 0.8868 - lr: 3.5481e-04\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.2305 - accuracy: 0.9169 - val_loss: 0.3111 - val_accuracy: 0.8872 - lr: 2.5119e-04\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2279 - accuracy: 0.9178 - val_loss: 0.3154 - val_accuracy: 0.8864 - lr: 1.7783e-04\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2288 - accuracy: 0.9175 - val_loss: 0.3142 - val_accuracy: 0.8856 - lr: 1.2589e-04\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2260 - accuracy: 0.9192 - val_loss: 0.3103 - val_accuracy: 0.8864 - lr: 8.9125e-05\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2280 - accuracy: 0.9186 - val_loss: 0.3168 - val_accuracy: 0.8844 - lr: 6.3096e-05\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2270 - accuracy: 0.9192 - val_loss: 0.3212 - val_accuracy: 0.8870 - lr: 4.4668e-05\n",
      "Evaluation after using learning rate scheduler\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3546 - accuracy: 0.8831\n",
      "[0.3546496629714966, 0.8830999732017517]\n"
     ]
    }
   ],
   "source": [
    "#OPTIMIZERS\n",
    "#Momentum optimizer\n",
    "\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "#momentum is beta value\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.001,momentum=0.9,nesterov=True)\n",
    "\n",
    "#Adagrad optimizer changes learning rate for every parameter:\n",
    "optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "#adagrad often slows down learning rate to a very small value\n",
    "\n",
    "optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001,rho=0.9)\n",
    "#improvement over Adagrad as it accumulates gradients of only recent iterations(sum of squared gradients for parameter)\n",
    "\n",
    "#Adam combines RMSprop and momentum\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.99)\n",
    "#beta_1 tells us how much of previous gradient information is retained in moving average\n",
    "#beta_2 tells us how much of previous squared gradient information(rmsprop) is retained in moving average\n",
    "\n",
    "optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9)\n",
    "#AdaMax focuses only on max of past squared gradients+ normal gradient so no beta_2\n",
    "\n",
    "optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001,beta_1=0.9,beta_2=0.99)\n",
    "#Nadam is Adam+nesterov\n",
    "\n",
    "#LEARNING RATE SCHEDULING\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, decay=1e-4) #performance scheduling\n",
    "#here decay=1/s, so after every 1000 instances learning rate changes\n",
    "\n",
    "def exponential_decay(epoch):\n",
    "    return 0.1*0.001**(epoch/20) #learnin rate changes after every 20 instances?\n",
    "#to use above function in callback:\n",
    "\n",
    "def exponential_decay(lr,s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr*0.001**(epoch/s)\n",
    "    return(exponential_decay_fn)\n",
    "\n",
    "print('Evaluation before using learning rate scheduler')\n",
    "print(model.evaluate(x_test_full,y_test_full))\n",
    "\n",
    "exponential_decay_fn=exponential_decay(lr=0.001,s=20)\n",
    "lr_scheduler=tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history=model.fit(x_train,y_train,validation_data=[x_valid,y_valid],epochs=10,callbacks=[lr_scheduler])\n",
    "print('Evaluation after using learning rate scheduler')\n",
    "print(model.evaluate(x_test_full,y_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9841afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGULARIZATION\n",
    "layer=tf.keras.layers.Dense(100,activation='relu',kernel_initializer='he_normal',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "#Dropout\n",
    "model=tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=[28,28]),\n",
    "                          tf.keras.layers.Dropout(rate=0.2),\n",
    "                          tf.keras.layers.Dense(100,activation='relu',kernel_initializer='he_normal'),\n",
    "                          tf.keras.layers.Dropout(rate=0.2),\n",
    "                          tf.keras.layers.Dense(100,activation='relu',kernel_initializer='he_normal'),\n",
    "                          tf.keras.layers.Dropout(rate=0.2),\n",
    "                          tf.keras.layers.Dense(10,activation='softmax')])\n",
    "#0.2 means 0.2 probability of each neuron in that layer being dropped or deactivated\n",
    "\n",
    "#MAX NORM regularization\n",
    "model_maxnorm=tf.keras.Sequential()\n",
    "model_maxnorm.add(tf.keras.layers.Flatten(input_shape=[28,28]))\n",
    "layer=tf.keras.layers.Dense(100,activation='relu',kernel_initializer='he_normal',\n",
    "                           kernel_constraint=tf.keras.constraints.max_norm(1.0))\n",
    "#kernel_constraint sets the maximum value of weight\n",
    "model_maxnorm.add(layer)\n",
    "model_maxnorm.add(tf.keras.layers.Dense(50,activation='relu',kernel_initializer='he_normal',\n",
    "                                       kernel_constraint=tf.keras.constraints.max_norm(2.0)))\n",
    "model_maxnorm.add(tf.keras.layers.Dense(10,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c0bd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created datasdet using tensorflow:\n",
      "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
      "Elements of dataset:\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "Elements of tensorflow dataset created from dictionary\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n",
      "Elements of repeated and batched dataset:\n",
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n",
      "Elements of dataset after map() function:\n",
      "tf.Tensor([ 0  1  4  9 16 25 36], shape=(7,), dtype=int32)\n",
      "tf.Tensor([49 64 81  0  1  4  9], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 25 36 49 64 81  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 1  4  9 16 25 36 49], shape=(7,), dtype=int32)\n",
      "tf.Tensor([64 81], shape=(2,), dtype=int32)\n",
      "Elements of dataset1 after using parallel calls concept:\n",
      "tf.Tensor([ 0  3  6  9 12  0  3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 6  9 12  0  3  6  9], shape=(7,), dtype=int32)\n",
      "tf.Tensor([12], shape=(1,), dtype=int32)\n",
      "Batches having sum greater than 13:\n",
      "tf.Tensor([ 0  3  6  9 12  0  3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 6  9 12  0  3  6  9], shape=(7,), dtype=int32)\n",
      "Taking only the first 2 batches from your dataset:\n",
      "tf.Tensor([ 0  1  4  9 16 25 36], shape=(7,), dtype=int32)\n",
      "tf.Tensor([49 64 81  0  1  4  9], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing using tensorflow\n",
    "x=tf.range(10)#creates 1D tensor containing values from 0 to 9\n",
    "dataset=tf.data.Dataset.from_tensor_slices(x) #creates dataset from the tensor x\n",
    "print('Created datasdet using tensorflow:')\n",
    "print(dataset)\n",
    "#dataset contains 10 zero diensional tensors of type tf.int32, shapes=() as each tensor of dataset is 0 dimensional\n",
    "#if individual tensor of dataset was a 1 dimensional tensor, then shapes would be length of the 1D tensor\n",
    "\n",
    "print('Elements of dataset:')\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "\n",
    "x_nested={'a':([1,2,3],[4,5,6]),'b':[7,8,9]}\n",
    "dataset=tf.data.Dataset.from_tensor_slices(x_nested)\n",
    "print('Elements of tensorflow dataset created from dictionary')\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "#(corresponding elements of all 3 lists stored as tensors)\n",
    "\n",
    "#CHAINING TRANSFORMATIONS\n",
    "dataset=tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset=dataset.repeat(3).batch(7) # repeat means dataset repeated 3 times, now it contains 30 zero dimensional tensors\n",
    "#batch(7) means each individual tensor in dataset is 1D tensor of max size 7\n",
    "print('Elements of repeated and batched dataset:')\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "\n",
    "#These dataset methods return new datasets\n",
    "def square_element(element):\n",
    "    return element**2\n",
    "\n",
    "\n",
    "dataset = dataset.map(square_element)\n",
    "print('Elements of dataset after map() function:')\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "    \n",
    "#if intensive computation is used via apply, we can distribute it across multiple threads\n",
    "def calc_sum(x):\n",
    "    return(x+(x*2))\n",
    "dataset1=tf.data.Dataset.from_tensor_slices(tf.range(5))\n",
    "dataset1=dataset1.repeat(3).batch(7)\n",
    "dataset1=dataset1.map(calc_sum,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#this lets tensorflow automatically decide number of threads\n",
    "\n",
    "print('Elements of dataset1 after using parallel calls concept:')\n",
    "for item in dataset1:\n",
    "    print(item)\n",
    "\n",
    "filtered_dataset=dataset1.filter(lambda x:tf.reduce_sum(x)>13)\n",
    "print('Batches having sum greater than 13:')\n",
    "for item in filtered_dataset:\n",
    "    print(item)\n",
    "\n",
    "print('Taking only the first 2 batches from your dataset:')\n",
    "for item in dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668edbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled dataset:\n",
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n",
      "1651\n"
     ]
    }
   ],
   "source": [
    "#SHUFFLING DATASET\n",
    "dataset=tf.data.Dataset.range(10).repeat(2)#dataset count of numbers is 20\n",
    "dataset=dataset.shuffle(buffer_size=4,seed=42).batch(7)\n",
    "import pandas as pd\n",
    "print('Shuffled dataset:')\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "housing=fetch_california_housing()\n",
    "x=housing.data\n",
    "y=housing.target\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "x_train,x_valid,y_train,y_valid=train_test_split(x_train,y_train,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "train_data=[(x_train,y_train)]\n",
    "valid_data=[(x_valid,y_valid)]\n",
    "test_data=[(x_test,y_test)]\n",
    "\n",
    "train_filepaths=['C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\train1.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\train2.csv',\n",
    "                   'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\train3.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\train4.csv',\n",
    "                   'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\train5.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\train6.csv',\n",
    "                   'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\train7.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\train8.csv']\n",
    "\n",
    "valid_filepaths=['C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\valid1.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\valid2.csv',\n",
    "                   'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\valid3.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\valid4.csv',\n",
    "                   'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\valid5.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\valid6.csv',\n",
    "                   'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\valid7.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\valid8.csv']\n",
    "\n",
    "test_filepaths=['C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\test1.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\test2.csv',\n",
    "                  'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\test3.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\test4.csv',\n",
    "                  'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\test5.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\test6.csv',\n",
    "                  'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\test7.csv', 'C:\\\\Personal\\\\ML\\\\csv files\\\\DL\\\\test8.csv']\n",
    "\n",
    "num_files=8\n",
    "num_instances_train=len(x_train)//num_files\n",
    "print(num_instances_train)\n",
    "num_instances_valid=len(x_valid)//num_files\n",
    "num_instances_test=len(x_test)//num_files\n",
    "\n",
    "for i in range(num_files):\n",
    "    start_index=i*num_instances_train\n",
    "    end_index=(i+1)*num_instances_train\n",
    "    subset_x_train=x_train[start_index:end_index]\n",
    "    subset_y_train=y_train[start_index:end_index]\n",
    "    subset_train_data=pd.DataFrame({'MedInc':subset_x_train[:,0],'HouseAge':subset_x_train[:,1],\n",
    "                                      'AveRooms':subset_x_train[:,2],'AveBedrms':subset_x_train[:,3],\n",
    "                                      'Population':subset_x_train[:,4],'AveOccup':subset_x_train[:,5],\n",
    "                                      'Latitude':subset_x_train[:,6],'Longitude':subset_x_train[:,7],\n",
    "                                      'MedianHouseValue':subset_y_train[:]})\n",
    "    subset_train_data.to_csv(train_filepaths[i],index=False)\n",
    "\n",
    "for i in range(num_files): #distributes to 8 csv files for validation set\n",
    "    start_index=i*num_instances_valid\n",
    "    end_index=(i+1)*num_instances_valid\n",
    "    subset_x_valid=x_valid[start_index:end_index]\n",
    "    subset_y_valid=y_valid[start_index:end_index]\n",
    "    subset_valid_data=pd.DataFrame({'MedInc':subset_x_valid[:,0],'HouseAge':subset_x_valid[:,1],\n",
    "                                      'AveRooms':subset_x_valid[:,2],'AveBedrms':subset_x_valid[:,3],\n",
    "                                      'Population':subset_x_valid[:,4],'AveOccup':subset_x_valid[:,5],\n",
    "                                      'Latitude':subset_x_valid[:,6],'Longitude':subset_x_valid[:,7],\n",
    "                                      'MedianHouseValue':subset_y_valid[:]})\n",
    "    subset_valid_data.to_csv(valid_filepaths[i],index=False)\n",
    "    \n",
    "for i in range(num_files): #distributes to 8 csv files for vtest set\n",
    "    start_index=i*num_instances_test\n",
    "    end_index=(i+1)*num_instances_test\n",
    "    subset_x_test=x_test[start_index:end_index]\n",
    "    subset_y_test=y_test[start_index:end_index]\n",
    "    subset_test_data=pd.DataFrame({'MedInc':subset_x_test[:,0],'HouseAge':subset_x_test[:,1],\n",
    "                                      'AveRooms':subset_x_test[:,2],'AveBedrms':subset_x_test[:,3],\n",
    "                                      'Population':subset_x_test[:,4],'AveOccup':subset_x_test[:,5],\n",
    "                                      'Latitude':subset_x_test[:,6],'Longitude':subset_x_test[:,7],\n",
    "                                      'MedianHouseValue':subset_y_test[:]})\n",
    "    subset_test_data.to_csv(test_filepaths[i],index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aabf63d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
      "Printing elements of shuffled dataset:\n",
      "tf.Tensor(b'2.1836,24.0,3.1074380165289255,1.0611570247933884,2646.0,4.373553719008265,33.98,-118.18,1.62', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.0707,52.0,4.92326139088729,1.1510791366906474,900.0,2.158273381294964,34.05,-118.38,4.179', shape=(), dtype=string)\n",
      "tf.Tensor(b'4.7069,27.0,6.523255813953488,1.1162790697674418,873.0,3.383720930232558,38.0,-120.97,1.769', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.0625,24.0,4.305084745762712,1.0084745762711864,465.0,1.9703389830508475,33.62,-117.89,0.938', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.4812,52.0,4.520689655172414,1.0689655172413792,707.0,2.4379310344827587,34.06,-118.34,4.328', shape=(), dtype=string)\n",
      "Mean and standard deviation of every column:\n",
      "[ 3.86893364e+00  2.85672647e+01  5.42040408e+00  1.09433536e+00\n",
      "  1.42691650e+03  3.02944025e+00  3.56468476e+01 -1.19583303e+02]\n",
      "First line of shuffled dataset after preprocessing:\n",
      "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
      "array([ 0.17957565,  1.2258911 , -0.04587359, -0.4651139 , -0.5108438 ,\n",
      "       -0.10096481,  0.8544316 , -1.3049603 ], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "filepaths_dataset=tf.data.Dataset.list_files(train_filepaths,seed=42)\n",
    "print(filepaths_dataset)\n",
    "#.list_files specifically creates dataset from filepaths\n",
    "n_readers=5\n",
    "import numpy as np\n",
    "shuffled_dataset=filepaths_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "                                             cycle_length=n_readers)\n",
    "#n_readers means 5 filepaths taken from filepaths_dataset, and each filepath is subjected to lambda function\n",
    "#TextLineDataset reads lines from each csv file, skip(1) skips column row\n",
    "#therefore interleave reads one line from every filepath, once filepath exhausted, new ones taken\n",
    "\n",
    "print('Printing elements of shuffled dataset:')\n",
    "for line in shuffled_dataset.take(5):\n",
    "    print(line)\n",
    "#returns 5 tensors, each containing a byte string\n",
    "\n",
    "x_mean=np.mean(x_train,axis=0)\n",
    "x_std=np.std(x_train,axis=0)\n",
    "print('Mean and standard deviation of every column:')\n",
    "print(x_mean)\n",
    "\n",
    "n_inputs=8\n",
    "#now we convert the byte string into numeric attributes\n",
    "def parse_csv_file(line):\n",
    "    defs=[0.]*n_inputs +[tf.constant([],dtype=tf.float32)]\n",
    "    #defs is a list(1D) with last element being a tensor(used to represent target value) [default values]\n",
    "    #regular list used to represent input features\n",
    "    fields=tf.io.decode_csv(line,record_defaults=defs)\n",
    "    #fields is a list of tensorflow tensors, one for every feature(defs used as deafult value if any missing value detected)\n",
    "    return tf.stack(fields[:-1]),tf.stack(fields[-1:])\n",
    "#tf.stack(fields[:-1]) stacks all input tensors into a single tensor\n",
    "\n",
    "def preprocess(line):\n",
    "    x,y=parse_csv_file(line)\n",
    "    x_standardized=(x-tf.constant(x_mean,dtype=tf.float32))/tf.constant(x_std,dtype=tf.float32)\n",
    "    return x_standardized,y\n",
    "#above methods help scale the features from the original byte string\n",
    "\n",
    "first_line=shuffled_dataset.take(1)\n",
    "print('First line of shuffled dataset after preprocessing:')\n",
    "print(preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'))\n",
    "\n",
    "\n",
    "#FUNCTION THAT CONSOLIDATES ALL ABOVE METHODS\n",
    "def csv_reader_dataset(filepaths,n_readers=5,n_read_threads=None,n_parse_threads=5,shuffle_buffer_size=10_000,\n",
    "                      seed=42,batch_size=32):\n",
    "    dataset=tf.data.Dataset.list_files(filepaths,seed=seed)\n",
    "    dataset=dataset.interleave(\n",
    "    lambda filepath:tf.data.TextLineDataset(filepath).skip(1),cycle_length=n_readers,num_parallel_calls=n_read_threads)\n",
    "    dataset=dataset.map(preprocess,num_parallel_calls=n_parse_threads)\n",
    "    #output of this line is tensor for every row containing numeric version of attributes\n",
    "    dataset=dataset.shuffle(shuffle_buffer_size,seed=seed)\n",
    "    return(dataset.batch(batch_size).prefetch(1))\n",
    "#prefetch makes sure that dataset is always 1 batch ahead\n",
    "#after batch_size 32 tensors stored in a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e42e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "413/413 [==============================] - 4s 6ms/step - loss: 0.7456 - val_loss: 0.5863\n",
      "Epoch 2/5\n",
      "413/413 [==============================] - 2s 5ms/step - loss: 0.4336 - val_loss: 0.4296\n",
      "Epoch 3/5\n",
      "413/413 [==============================] - 2s 5ms/step - loss: 0.3905 - val_loss: 0.4303\n",
      "Epoch 4/5\n",
      "413/413 [==============================] - 2s 5ms/step - loss: 0.3745 - val_loss: 0.3942\n",
      "Epoch 5/5\n",
      "413/413 [==============================] - 2s 4ms/step - loss: 0.3643 - val_loss: 0.3835\n",
      "129/129 [==============================] - 1s 3ms/step - loss: 0.3763\n",
      "Evaluation metrics on test set:\n",
      "0.3762616813182831\n",
      "3/3 [==============================] - 0s 23ms/step\n",
      "Predictions on new_set:\n",
      "[[1.3673065]\n",
      " [2.315952 ]\n",
      " [0.8822606]\n",
      " [2.8477812]\n",
      " [1.5144815]]\n"
     ]
    }
   ],
   "source": [
    "train_set=csv_reader_dataset(train_filepaths)\n",
    "valid_set=csv_reader_dataset(valid_filepaths)\n",
    "test_set=csv_reader_dataset(test_filepaths)\n",
    "\n",
    "model=tf.keras.Sequential([tf.keras.layers.Dense(64,activation='relu',input_shape=(8,)),\n",
    "                          tf.keras.layers.Dense(32,activation='relu'),\n",
    "                          tf.keras.layers.Dense(1)])\n",
    "model.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "model.fit(train_set,validation_data=valid_set,epochs=5)\n",
    "\n",
    "test_mse=model.evaluate(test_set)\n",
    "print('Evaluation metrics on test set:')\n",
    "print(test_mse)\n",
    "new_set=test_set.take(3)\n",
    "y_pred=model.predict(new_set)\n",
    "print('Predictions on new_set:')\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ef8ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing elements of TFR dataset:\n",
      "tf.Tensor(b'Example record', shape=(), dtype=string)\n",
      "tf.Tensor(b'Example 2 record', shape=(), dtype=string)\n",
      "Epoch 1/5\n",
      "413/413 [==============================] - 3s 4ms/step - loss: 2.4756 - val_loss: 0.9646\n",
      "Epoch 2/5\n",
      "413/413 [==============================] - 1s 3ms/step - loss: 0.7431 - val_loss: 0.6962\n",
      "Epoch 3/5\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.6358 - val_loss: 0.6499\n",
      "Epoch 4/5\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.6015 - val_loss: 0.6142\n",
      "Epoch 5/5\n",
      "413/413 [==============================] - 1s 2ms/step - loss: 0.5767 - val_loss: 0.5980\n",
      "Predictions after creating another sequential model combining preprocessing anf prev model\n",
      "tf.Tensor(\n",
      "[[-19.512403]\n",
      " [-19.23635 ]\n",
      " [-19.00037 ]], shape=(3, 1), dtype=float32)\n",
      "Trainset after normalization:\n",
      "<MapDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "#TFRecord\n",
    "#TFRecord stores data in binary format, the record have varying sizes\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"Example record\")\n",
    "    f.write(b\"Example 2 record\")\n",
    "\n",
    "filepaths=[\"my_data.tfrecord\"]\n",
    "dataset=tf.data.TFRecordDataset(filepaths)\n",
    "print('Printing elements of TFR dataset:')\n",
    "for item in dataset:\n",
    "    print(item)\n",
    "\n",
    "#KERAS PREPROCESSING\n",
    "norm_layer=tf.keras.layers.Normalization()\n",
    "model=tf.keras.Sequential([norm_layer,tf.keras.layers.Dense(1)])\n",
    "model.compile(loss='mse',optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "norm_layer.adapt(x_train)#helps compute mean and standard deviation for normalization\n",
    "model.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=5)\n",
    "#in this approach preprocessing code is the same for both training and production, done for every epoch\n",
    "\n",
    "#instead of preprocessing every epoch, we can do it before training only once\n",
    "norm_layer=tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(x_train)\n",
    "x_train_scaled=norm_layer(x_train)\n",
    "x_valid_scaled=norm_layer(x_valid)\n",
    "#however this approach does not do preprocessing when in production\n",
    "\n",
    "final_model=tf.keras.Sequential([norm_layer,model])\n",
    "x_new=x_test[:3]\n",
    "y_pred=final_model(x_new)\n",
    "print('Predictions after creating another sequential model combining preprocessing anf prev model')\n",
    "print(y_pred)\n",
    "\n",
    "#we adapt the normalization layer to a regular dataset, we can do this with a tensorflow dataset as well\n",
    "#norm_layer.adapt(train_set)\n",
    "train_set=train_set.map(lambda x,y:(norm_layer(x),y))# here you don't need to adapt\n",
    "#x represents input features, y represents target\n",
    "print('Trainset after normalization:')\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0276d7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors age:\n",
      "tf.Tensor(\n",
      "[[10.]\n",
      " [93.]\n",
      " [57.]\n",
      " [18.]\n",
      " [37.]\n",
      " [ 5.]], shape=(6, 1), dtype=float32)\n",
      "Age after discretization:\n",
      "tf.Tensor(\n",
      "[[0]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]], shape=(6, 1), dtype=int64)\n",
      "Individual tensors in age_categories:\n",
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "tf.Tensor([0], shape=(1,), dtype=int64)\n",
      "Age categories after specifying only number of bins:\n",
      "tf.Tensor(\n",
      "[[1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [0]], shape=(6, 1), dtype=int64)\n",
      "After doing CategoryEncoding on age_categories\n",
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]], shape=(6, 3), dtype=float32)\n",
      "Conversion of two dimensional age categories to CategoryEncoding:\n",
      "tf.Tensor(\n",
      "[[1. 1. 0.]\n",
      " [0. 0. 2.]\n",
      " [1. 0. 1.]], shape=(3, 3), dtype=float32)\n",
      "Result of stringlookup on new cities data:\n",
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [3]\n",
      " [0]], shape=(4, 1), dtype=int64)\n",
      "One Hot Encoding result:\n",
      "tf.Tensor(\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n",
      "WARNING:tensorflow:5 out of the last 417 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x00000202029095E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Result of string lookup layer after adding num_oov_indices:\n",
      "tf.Tensor(\n",
      "[[5]\n",
      " [7]\n",
      " [4]\n",
      " [3]\n",
      " [4]], shape=(5, 1), dtype=int64)\n",
      "Results of hashing layer on city names:\n",
      "tf.Tensor(\n",
      "[[0]\n",
      " [9]\n",
      " [9]\n",
      " [1]], shape=(4, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#DISCRETIZATION\n",
    "age=tf.constant([[10.],[93.],[57.],[18.],[37.],[5.]])\n",
    "print('Tensors age:')\n",
    "print(age)\n",
    "discretize_layer=tf.keras.layers.Discretization(bin_boundaries=[18.,50.])\n",
    "age_categories=discretize_layer(age)\n",
    "print('Age after discretization:')\n",
    "print(age_categories)\n",
    "print('Individual tensors in age_categories:')\n",
    "for item in age_categories:\n",
    "    print(item)\n",
    "\n",
    "#in above snippet we specify the bins\n",
    "discretize_layer=tf.keras.layers.Discretization(num_bins=3)\n",
    "discretize_layer.adapt(age)\n",
    "age_categories=discretize_layer(age)\n",
    "print('Age categories after specifying only number of bins:')\n",
    "print(age_categories)\n",
    "\n",
    "#Category encoding(similar to OneHotEncoder)\n",
    "onehot_layer=tf.keras.layers.CategoryEncoding(num_tokens=3)#here there are 3 categories\n",
    "print('After doing CategoryEncoding on age_categories')\n",
    "print(onehot_layer(age_categories))\n",
    "\n",
    "onehot_layer=tf.keras.layers.CategoryEncoding(num_tokens=3,output_mode='count')\n",
    "two_age_categories=np.array([[1,0],[2,2],[2,0]])\n",
    "print('Conversion of two dimensional age categories to CategoryEncoding:')\n",
    "print(onehot_layer(two_age_categories))\n",
    "#here even though a numpy array is given as input, tensorflow converts it to tensor because layers present\n",
    "\n",
    "#STRINGLOOKUP\n",
    "cities=['Auckland','Paris','Paris','San Fransisco']\n",
    "str_lookup=tf.keras.layers.StringLookup()\n",
    "str_lookup.adapt(cities) # helps identify how many categories there are\n",
    "str_lookup_res=str_lookup([['Paris'],['Paris'],['Auckland'],['Montreal']])#unknown layers classified as 0\n",
    "\n",
    "print('Result of stringlookup on new cities data:')\n",
    "print(str_lookup_res)\n",
    "\n",
    "#For one hot string lookup\n",
    "str_lookup_hot=tf.keras.layers.StringLookup(output_mode='one_hot')\n",
    "str_lookup_hot.adapt(cities)\n",
    "str_lookup_res=str_lookup_hot([['Paris'],['Paris'],['Auckland'],['Montreal']])\n",
    "print('One Hot Encoding result:')\n",
    "print(str_lookup_res)\n",
    "\n",
    "str_lookup_layer=tf.keras.layers.StringLookup(num_oov_indices=5)#this doesn't assign all unknown categories as 0\n",
    "#can handle upto 5 unexpected ccategories\n",
    "str_lookup_layer.adapt(cities)\n",
    "print('Result of string lookup layer after adding num_oov_indices:')\n",
    "print(str_lookup_layer([[\"Paris\"],[\"Auckland\"],[\"Foo\"],[\"Bar\"],[\"Baz\"]]))\n",
    "#since num_oov_indices=5, Paris gets classified as 5\n",
    "#the unknown ones are classified to category numbers<5\n",
    "\n",
    "#HASHING LAYER\n",
    "hashing_layer=tf.keras.layers.Hashing(num_bins=10)\n",
    "hashing_res=hashing_layer([[\"Paris\"],[\"Auckland\"],['Auckland'],['Poland']])\n",
    "print('Results of hashing layer on city names:')\n",
    "print(hashing_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9e5ea2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of embedding:\n",
      "tf.Tensor(\n",
      "[[-0.04512222  0.04537706]\n",
      " [-0.04893095  0.04836159]\n",
      " [-0.04512222  0.04537706]], shape=(3, 2), dtype=float32)\n",
      "Result of combining lookup and embedding keras layers:\n",
      "tf.Tensor(\n",
      "[[[ 0.00085356 -0.01974626]]\n",
      "\n",
      " [[ 0.00085356 -0.01974626]]\n",
      "\n",
      " [[ 0.00085356 -0.01974626]]], shape=(3, 1, 2), dtype=float32)\n",
      "Results of text vectorization:\n",
      "tf.Tensor(\n",
      "[[3 1 0]\n",
      " [2 4 6]], shape=(2, 3), dtype=int64)\n",
      "TF_IDF RESULTS:\n",
      "tf.Tensor(\n",
      "[[3 1 0 0]\n",
      " [6 3 1 3]], shape=(2, 4), dtype=int64)\n",
      "Results from the pre trained model:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'round'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m sentence_embeddings\u001b[38;5;241m=\u001b[39mhub_layer(tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot to be\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults from the pre trained model:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msentence_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'round'"
     ]
    }
   ],
   "source": [
    "#EMBEDDING\n",
    "tf.random.set_seed(42)\n",
    "embedding_layer=tf.keras.layers.Embedding(input_dim=5,output_dim=2)\n",
    "#input_dim specifies max no of catgroy IDs, lesser number can also be used\n",
    "print('Results of embedding:')\n",
    "print(embedding_layer(np.array([2,4,2])))\n",
    "#here 2,4,2 are category IDs\n",
    "#there is one row per category and one column per dimension in sparse matrix\n",
    "ocean_prox = [['NEAR BAY'], ['NEAR OCEAN'], ['INLAND'], ['ISLAND']]\n",
    "str_lookup = tf.keras.layers.StringLookup(num_oov_indices=5)\n",
    "str_lookup.adapt(ocean_prox)\n",
    "\n",
    "# Encode string categories into integers\n",
    "encoded_inputs=str_lookup(np.array([['NEAR OCEAN'],['ISLAND'],['NEAR BAY']]))\n",
    "encoded_inputs=tf.clip_by_value(encoded_inputs,0,4)  # Clip indices to [0, input_dim)\n",
    "\n",
    "lookup_embed=tf.keras.Sequential([tf.keras.layers.Embedding(input_dim=str_lookup.vocabulary_size()\n",
    "                                                            ,output_dim=2)])\n",
    "results=lookup_embed(encoded_inputs)\n",
    "print('Result of combining lookup and embedding keras layers:')\n",
    "print(results)\n",
    "\n",
    "#TEXT PREPROCESSING\n",
    "sentence_data=[\"To be\",\"!(to be)\",\"Be,be,be\",\"That's the question\"]\n",
    "text_vec_layer=tf.keras.layers.TextVectorization()\n",
    "text_vec_layer.adapt(sentence_data)\n",
    "\n",
    "text_results=text_vec_layer(['Be good!','To the question'])\n",
    "print('Results of text vectorization:')\n",
    "print(text_results)\n",
    "#unknown words get encoded as 1, 0 is padded as first input is shorter than second\n",
    "#tf idf gives importance(weight of aparticular word)\n",
    "\n",
    "text_vec_Layer=tf.keras.layers.TextVectorization(output_mode='tf_idf')\n",
    "text_vec_layer.adapt(sentence_data)\n",
    "print('TF_IDF RESULTS:')\n",
    "print(text_vec_layer([\"Be good!\",\"Question: be or be?\"]))\n",
    "#the more a word appears, the lesser the tf_idf score\n",
    "\n",
    "#USING PRE TRAINED MODEL FROM TENSOR HUB\n",
    "import tensorflow_hub as hub\n",
    "hub_layer=hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "#this layer outputs roughly 50 dimensions\n",
    "sentence_embeddings=hub_layer(tf.constant([\"To be\",\"Not to be\"]))\n",
    "print('Results from the pre trained model:')\n",
    "print(sentence_embeddings.numpy.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd3c2cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[174, 201, 231],\n",
       "         [174, 201, 231],\n",
       "         [174, 201, 231],\n",
       "         ...,\n",
       "         [250, 251, 255],\n",
       "         [250, 251, 255],\n",
       "         [250, 251, 255]],\n",
       " \n",
       "        [[172, 199, 229],\n",
       "         [173, 200, 230],\n",
       "         [173, 200, 230],\n",
       "         ...,\n",
       "         [251, 252, 255],\n",
       "         [251, 252, 255],\n",
       "         [251, 252, 255]],\n",
       " \n",
       "        [[174, 201, 231],\n",
       "         [174, 201, 231],\n",
       "         [174, 201, 231],\n",
       "         ...,\n",
       "         [252, 253, 255],\n",
       "         [252, 253, 255],\n",
       "         [252, 253, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 88,  80,   7],\n",
       "         [147, 138,  69],\n",
       "         [122, 116,  38],\n",
       "         ...,\n",
       "         [ 39,  42,  33],\n",
       "         [  8,  14,   2],\n",
       "         [  6,  12,   0]],\n",
       " \n",
       "        [[122, 112,  41],\n",
       "         [129, 120,  53],\n",
       "         [118, 112,  36],\n",
       "         ...,\n",
       "         [  9,  12,   3],\n",
       "         [  9,  15,   3],\n",
       "         [ 16,  24,   9]],\n",
       " \n",
       "        [[116, 103,  35],\n",
       "         [104,  93,  31],\n",
       "         [108, 102,  28],\n",
       "         ...,\n",
       "         [ 43,  49,  39],\n",
       "         [ 13,  21,   6],\n",
       "         [ 15,  24,   7]]], dtype=uint8),\n",
       " array([[[ 2, 19, 13],\n",
       "         [ 3, 18, 13],\n",
       "         [ 7, 20, 13],\n",
       "         ...,\n",
       "         [ 1, 77, 64],\n",
       "         [ 0, 76, 64],\n",
       "         [ 0, 75, 63]],\n",
       " \n",
       "        [[ 1, 18, 12],\n",
       "         [ 3, 18, 13],\n",
       "         [ 7, 20, 13],\n",
       "         ...,\n",
       "         [ 0, 76, 64],\n",
       "         [ 1, 74, 65],\n",
       "         [ 1, 74, 65]],\n",
       " \n",
       "        [[ 2, 17, 12],\n",
       "         [ 6, 19, 12],\n",
       "         [ 7, 20, 13],\n",
       "         ...,\n",
       "         [ 1, 74, 65],\n",
       "         [ 1, 74, 67],\n",
       "         [ 1, 74, 67]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0, 46, 40],\n",
       "         [ 1, 48, 40],\n",
       "         [ 1, 47, 37],\n",
       "         ...,\n",
       "         [ 5, 44, 26],\n",
       "         [ 6, 43, 26],\n",
       "         [ 7, 44, 27]],\n",
       " \n",
       "        [[ 0, 47, 41],\n",
       "         [ 1, 48, 40],\n",
       "         [ 1, 47, 37],\n",
       "         ...,\n",
       "         [ 6, 45, 27],\n",
       "         [ 7, 44, 27],\n",
       "         [ 7, 44, 27]],\n",
       " \n",
       "        [[ 0, 47, 41],\n",
       "         [ 1, 48, 40],\n",
       "         [ 0, 46, 36],\n",
       "         ...,\n",
       "         [ 7, 46, 28],\n",
       "         [ 8, 45, 28],\n",
       "         [ 9, 43, 27]]], dtype=uint8)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMAGE PREPROCESSING LAYERS\n",
    "from sklearn.datasets import load_sample_images\n",
    "images=load_sample_images()['images']#multi dim array\n",
    "crop_image_layer=tf.keras.layers.CenterCrop(height=100,width=100)\n",
    "cropped_images=crop_image_layer(images)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "datasets=tfds.load(name='mnist')\n",
    "mnist_train,mnist_test=datasets['train'],datasets['test']\n",
    "\n",
    "for batch in mnist_train.shuffle(10_000,seed=42).batch(32).prefetch(1):\n",
    "    images=batch['image']\n",
    "    label=batch['label']\n",
    "#each item here is a dictionary containing features and labels\n",
    "#for keras it should ideally be a tuple containing 2 elements\n",
    "mnist_train=mnist_train.shuffle(10_000,seed=42).batch(32)\n",
    "mnist_train=mnist_train.map(lambda items: (items['image'],items['label']))\n",
    "mnist_train=mnist_train.prefetch(1)\n",
    "\n",
    "#you can directly do this while loading if you use as_supervised=True\n",
    "train_set,valid_set,test_set=tfds.load(name='mnist',\n",
    "                                      split=[\"train[:90%]\",\"train[90%:]\",\"test\"],\n",
    "                                      as_supervised=True)\n",
    "train_set=train_set.shuffle(10_000,seed=42).batch(32).prefetch(1)\n",
    "valid_set=valid_set.batch(32).cache()\n",
    "test_set=test_set.batch(32).cache()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model=tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "                          tf.keras.layers.Dense(10,activation='softmax')])\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='nadam',metrics=['accuracy'])\n",
    "history=model.fit(train_set,validation_data=valid_set,epochs=5)\n",
    "print('Test set evaluation:')\n",
    "print(model.evaluate(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c1d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
